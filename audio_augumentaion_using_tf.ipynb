{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "\n",
    "from util.config import Config\n",
    "from util.text import text_to_char_array\n",
    "\n",
    "\n",
    "def read_csvs(csv_files):\n",
    "    source_data = None\n",
    "    for csv in csv_files:\n",
    "        file = pandas.read_csv(csv, encoding='utf-8', na_filter=False)\n",
    "        #FIXME: not cross-platform\n",
    "        csv_dir = os.path.dirname(os.path.abspath(csv))\n",
    "        file['wav_filename'] = file['wav_filename'].str.replace(r'(^[^/])', lambda m: os.path.join(csv_dir, m.group(1))) # pylint: disable=cell-var-from-loop\n",
    "        if source_data is None:\n",
    "            source_data = file\n",
    "        else:\n",
    "            source_data = source_data.append(file)\n",
    "    return source_data\n",
    "\n",
    "\n",
    "def samples_to_mfccs(samples, sample_rate):\n",
    "    spectrogram = contrib_audio.audio_spectrogram(samples,\n",
    "                                                  window_size=Config.audio_window_samples,\n",
    "                                                  stride=Config.audio_step_samples,\n",
    "                                                  magnitude_squared=True)\n",
    "    mfccs = contrib_audio.mfcc(spectrogram, sample_rate, dct_coefficient_count=Config.n_input)\n",
    "    mfccs = tf.reshape(mfccs, [-1, Config.n_input])\n",
    "\n",
    "    return mfccs, tf.shape(mfccs)[0]\n",
    "\n",
    "\n",
    "def audiofile_to_features(wav_filename,noise_filename=None):\n",
    "    if noise_filename:\n",
    "        samples = tf.read_file(wav_filename)\n",
    "        noise_samples = tf.read_file(noise_filename)\n",
    "        noise_decoded = contrib_audio.decode_wav(noise_samples, desired_channels=1)\n",
    "        if len(decoded.audio)>len(noise_decoded) and decoded.sample_rate!=noise_decoded.sample_rate:\n",
    "            decoded_audio=decoded.audio\n",
    "        else:\n",
    "            decoded_audio=decoded.audio+noise_decoded[:len(decoded.audio)]\n",
    "        features, features_len = samples_to_mfccs(decoded_audio, decoded.sample_rate)\n",
    "    else:\n",
    "        samples = tf.read_file(wav_filename)\n",
    "        decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n",
    "        features, features_len = samples_to_mfccs(decoded.audio, decoded.sample_rate)\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def entry_to_features(wav_filename,noise_filename, transcript):\n",
    "    # https://bugs.python.org/issue32117\n",
    "    features, features_len = audiofile_to_features(wav_filename,noise_filename)\n",
    "    return features, features_len, tf.SparseTensor(*transcript)\n",
    "\n",
    "\n",
    "def to_sparse_tuple(sequence):\n",
    "    r\"\"\"Creates a sparse representention of ``sequence``.\n",
    "        Returns a tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = np.asarray(list(zip([0]*len(sequence), range(len(sequence)))), dtype=np.int64)\n",
    "    shape = np.asarray([1, len(sequence)], dtype=np.int64)\n",
    "    return indices, sequence, shape\n",
    "\n",
    "\n",
    "def create_dataset(csvs, batch_size, cache_path=''):\n",
    "    df = read_csvs(csvs)\n",
    "    df.sort_values(by='wav_filesize', inplace=True)\n",
    "\n",
    "    # Convert to character index arrays\n",
    "    df['transcript'] = df['transcript'].apply(partial(text_to_char_array, alphabet=Config.alphabet))\n",
    "\n",
    "    def generate_values():\n",
    "        if \"noise_filename\" in df.columns:                        \n",
    "            for _, row in df.iterrows():\n",
    "                yield row.wav_filename,row.noise_filename,to_sparse_tuple(row.transcript)\n",
    "        else:\n",
    "            for _, row in df.iterrows():\n",
    "                yield row.wav_filename,to_sparse_tuple(row.transcript)                                  \n",
    "\n",
    "    # Batching a dataset of 2D SparseTensors creates 3D batches, which fail\n",
    "    # when passed to tf.nn.ctc_loss, so we reshape them to remove the extra\n",
    "    # dimension here.\n",
    "    def sparse_reshape(sparse):\n",
    "        shape = sparse.dense_shape\n",
    "        return tf.sparse.reshape(sparse, [shape[0], shape[2]])\n",
    "\n",
    "    def batch_fn(features, features_len, transcripts):\n",
    "        features = tf.data.Dataset.zip((features, features_len))\n",
    "        features = features.padded_batch(batch_size,\n",
    "                                         padded_shapes=([None, Config.n_input], []))\n",
    "        transcripts = transcripts.batch(batch_size).map(sparse_reshape)\n",
    "        return tf.data.Dataset.zip((features, transcripts))\n",
    "\n",
    "    num_gpus = len(Config.available_devices)\n",
    "\n",
    "    dataset = (tf.data.Dataset.from_generator(generate_values,\n",
    "                                              output_types=(tf.string, (tf.int64, tf.int32, tf.int64)))\n",
    "                              .map(entry_to_features, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                              .cache(cache_path)\n",
    "                              .window(batch_size, drop_remainder=True).flat_map(batch_fn)\n",
    "                              .prefetch(num_gpus))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def secs_to_hours(secs):\n",
    "    hours, remainder = divmod(secs, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return '%d:%02d:%02d' % (hours, minutes, seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiofile_to_features(wav_filename,noise_filename=None):\n",
    "    if noise_filename is not None:\n",
    "        samples = tf.read_file(wav_filename)\n",
    "        noise_samples = tf.read_file(noise_filename)\n",
    "        decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n",
    "        noise_decoded = contrib_audio.decode_wav(noise_samples, desired_channels=1)\n",
    "        '''\n",
    "        if len(decoded.audio)>len(noise_decoded.audio) and decoded.sample_rate != noise_decoded.sample_rate:\n",
    "            decoded_audio=decoded.audio\n",
    "        else:\n",
    "            decoded_audio = tf.add(decoded.audio+noise_decoded.audio[:len(decoded.audio.eval())])\n",
    "            features, features_len = samples_to_mfccs(decoded_audio, decoded.sample_rate)\n",
    "            print(\"failed if noise\")\n",
    "        '''\n",
    "        print(\"SAMPLE RTATE SIZE::-------------\")\n",
    "        print(tf.size(decoded.sample_rate))\n",
    "        print(tf.size(noise_decoded.sample_rate))\n",
    "\n",
    "        if_true_cond = tf.cond(tf.size(decoded.audio) > tf.size(noise_decoded.audio), lambda : decoded.audio, lambda : tf.add(decoded.audio, tf.slice(noise_decoded.audio,[0,0],tf.unstack(tf.shape(decoded.audio)))))\n",
    "        \n",
    "        print(tf.shape(tf.slice(noise_decoded.audio,[0,0],tf.unstack(tf.shape(decoded.audio)))))\n",
    "        print(tf.shape(decoded.audio))\n",
    "        \n",
    "        decoded_audio = tf.cond(tf.equal(decoded.sample_rate, noise_decoded.sample_rate), lambda: if_true_cond, lambda: decoded.audio)\n",
    "        decoded_audio = tf.identity(decoded_audio, name=\"input_with_noise_audio\")\n",
    "        features, features_len = samples_to_mfccs(decoded_audio, decoded.sample_rate)\n",
    "        \n",
    "    else:\n",
    "        samples = tf.read_file(wav_filename)\n",
    "        decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n",
    "        features, features_len = samples_to_mfccs(decoded.audio, decoded.sample_rate)\n",
    "\n",
    "    return features, features_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/projects/datasets/LibriSpeech/test-clean-wav/7127-75947-0000.wav'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile_to_features(read_csvs([csv_path])[\"wav_filename\"].iloc[:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = tf.read_file(\"/home/ubuntu/projects/datasets/LibriSpeech/test-clean-wav/7127-75947-0000.wav\")\n",
    "decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n",
    "samples1 = tf.read_file(\"/home/ubuntu/projects/datasets/noise_wav/00noice.wav\")\n",
    "noise_decoded = contrib_audio.decode_wav(samples1, desired_channels=1)\n",
    "\n",
    "print(tf.size(decoded.sample_rate))\n",
    "print(tf.size(noise_decoded.sample_rate))\n",
    "\n",
    "if_true_cond = tf.cond(tf.size(decoded.audio) > tf.size(noise_decoded.audio), lambda : decoded.audio, lambda : tf.add(decoded.audio, tf.slice(noise_decoded.audio,[0,0],tf.unstack(tf.shape(decoded.audio)))))\n",
    "\n",
    "print(tf.shape(tf.slice(noise_decoded.audio,[0,0],tf.unstack(tf.shape(decoded.audio)))))\n",
    "print(tf.shape(decoded.audio))\n",
    "\n",
    "decoded_audio = tf.cond(tf.equal(decoded.sample_rate, noise_decoded.sample_rate), lambda: if_true_cond, lambda: decoded.audio)\n",
    "decoded_audio = tf.identity(decoded_audio, name=\"input_with_noise_audio\")\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(type(decoded.sample_rate.eval()),type(noise_decoded.sample_rate.eval()))\n",
    "    print(np.dtype(decoded.audio.eval()[0][0]))\n",
    "    print(len(decoded.audio.eval()),len(noise_decoded.audio.eval()),len(decoded_audio.eval()))\n",
    "    print(decoded.audio.eval().ravel().reshape((len(noise_decoded.audio.eval().ravel()),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Size_1:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Size_2:0\", shape=(), dtype=int32)\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"Shape_2:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'> <class 'numpy.int32'>\n",
      "float32\n",
      "287520 2850048 287520\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 287520 into shape (2850048,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5350d4b7d543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_decoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_decoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 287520 into shape (2850048,1)"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "#tf.Print(decoded.sample_rate)\n",
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(type(decoded.sample_rate.eval()),type(noise_decoded.sample_rate.eval()))\n",
    "    print(np.dtype(decoded.audio.eval()[0][0]))\n",
    "    print(len(decoded.audio.eval()),len(noise_decoded.audio.eval()),len(decoded_audio.eval()))\n",
    "    print(decoded.audio.eval().ravel().reshape((len(noise_decoded.audio.eval().ravel()),1)))\n",
    "    \n",
    "# with sess.as_default():\n",
    "#     print(decoded1.sample_rate.eval())\n",
    "#     print(type(decoded1.audio.eval()))\n",
    "#     print(len(decoded1.audio.eval()))\n",
    "#     print(decoded.audio.eval().ravel()+decoded1.audio.eval().ravel()[:len(decoded.audio.eval())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_samples_to_int16(y):\n",
    "    \"\"\"Convert floating-point numpy array of audio samples to int16.\"\"\"\n",
    "    if not issubclass(y.dtype.type, np.floating):\n",
    "        raise ValueError('input samples not floating-point')\n",
    "    return (y * np.iinfo(np.int16).max).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()  \n",
    "with sess.as_default():\n",
    "    print(decoded1.sample_rate.eval())\n",
    "    print(decoded1.audio.eval())\n",
    "    print(len(decoded1.audio.eval()))\n",
    "    print(decoded.audio.eval().ravel()+decoded1.audio.eval().ravel()[:len(decoded.audio.eval())])\n",
    "    print(samples_to_wav_data(decoded.audio.eval().ravel()+decoded1.audio.eval().ravel()[:len(decoded.audio.eval())],16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import tempfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "import six\n",
    "def samples_to_wav_data(samples, sample_rate):\n",
    "    \"\"\"Converts floating point samples to wav data.\"\"\"\n",
    "    wav_io = six.BytesIO()\n",
    "    scipy.io.wavfile.write(\"ddddddddddddd.wav\", sample_rate, float_samples_to_int16(samples))\n",
    "    return wav_io.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge CVS with Audio augumentation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/projects/datasets/cv-valid-dev.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-dev-other.csv',\n",
       " '/home/ubuntu/projects/datasets/cv-valid-test.csv',\n",
       " '/home/ubuntu/projects/datasets/ldc93s1.csv',\n",
       " '/home/ubuntu/projects/datasets/ted-test.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-train-other-500.csv',\n",
       " '/home/ubuntu/projects/datasets/voxforge-test.csv',\n",
       " '/home/ubuntu/projects/datasets/cv-valid-train.csv',\n",
       " '/home/ubuntu/projects/datasets/cv-other-dev.csv',\n",
       " '/home/ubuntu/projects/datasets/voxforge-dev.csv',\n",
       " '/home/ubuntu/projects/datasets/ted-dev.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-dev-clean.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-test-other.csv',\n",
       " '/home/ubuntu/projects/datasets/cv-other-train.csv',\n",
       " '/home/ubuntu/projects/datasets/cv-other-test.csv',\n",
       " '/home/ubuntu/projects/datasets/voxforge-train.csv',\n",
       " '/home/ubuntu/projects/datasets/cv-invalid.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-train-clean-360.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-train-clean-100.csv',\n",
       " '/home/ubuntu/projects/datasets/ted-train.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-test-clean.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root=\"/home/ubuntu/projects/datasets\"\n",
    "csv_paths=glob.glob(root+\"/*.csv\")\n",
    "csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/projects/datasets/cv-valid-train.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-train-clean-360.csv',\n",
       " '/home/ubuntu/projects/datasets/librivox-train-clean-100.csv',\n",
       " '/home/ubuntu/projects/datasets/ted-train.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv=[]\n",
    "test_csv=[]\n",
    "val_csv=[]\n",
    "for i in csv_paths:\n",
    "    if \"valid-train\" in i:\n",
    "        train_csv.append(i)\n",
    "    if 'train-clean' in i:\n",
    "        train_csv.append(i)\n",
    "    if \"ted-train\" in i:\n",
    "        train_csv.append(i)\n",
    "#     if \"voxforge-train\" in i:\n",
    "#         train_csv.append(i)\n",
    "    if \"test\" in i:\n",
    "        test_csv.append(i)\n",
    "    if \"dev\" in i:\n",
    "        val_csv.append(i)\n",
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file  \n",
    "import random\n",
    "\n",
    "def merge_csv_with_noise(csv_list,csv_type,noise_dir=None):\n",
    "\n",
    "    for j in range(len(csv_list)):\n",
    "        if j>0:\n",
    "            df=pd.read_csv(csv_list[j])\n",
    "            df1= df1.append(df, ignore_index=True)\n",
    "        else:\n",
    "            df1=pd.read_csv(csv_list[j])\n",
    "    if noise_dir:\n",
    "        noise_files=glob.glob(noise_dir+\"/*.wav\")\n",
    "        df1['noise_filename'] = df1.apply(lambda x: random.choice(noise_files), axis=1)\n",
    "    else:\n",
    "        print(\"noise_dir not found: merge without noise file\")\n",
    "        \n",
    "    df1.to_csv(csv_type+\"_csv_final.csv\", index=False)\n",
    "    print(len(df1))\n",
    "    #print(df1.iloc[:48])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418810\n"
     ]
    }
   ],
   "source": [
    "noise_dir=\"/home/ubuntu/projects/datasets/noise_data_30_sec\"\n",
    "merge_csv_with_noise(train_csv,\"train\",noise_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ubuntu/projects/datasets/ted-test.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "for i in range(len(df)):\n",
    "    my_file = Path(df.iloc[i][\"wav_filename\"])\n",
    "    if my_file.is_file():\n",
    "        pass\n",
    "    else:\n",
    "        print(i,df.iloc[0][\"wav_filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0][\"wav_filename\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of noise fiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_files=glob.glob(\"/home/ubuntu/projects/datasets/noise_data_30_sec/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxy(noise_files):\n",
    "    return \n",
    "df1['noise_filename'] = df1.apply(lambda x: random.choice(noise_files), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "trainX = np.linspace(-1, 1, 101)\n",
    "trainY = 3 * trainX + np.random.randn(*trainX.shape) * 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = tf.Variable(0.0, name=\"weights\")\n",
    "y_model = tf.multiply(X, w)\n",
    " \n",
    "cost = (tf.pow(Y-y_model, 2))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init= tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(100):\n",
    "        for (x, y) in zip(trainX, trainY):\n",
    "            sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x has shape [2, 3, 2]\n",
    "x = tf.constant([[[1., 2.], [3., 4. ], [5. , 6. ]],\n",
    "                 [[7., 8.], [9., 10.], [11., 12.]]])\n",
    "\n",
    "# Extracts x[0, 1:2, :] == [[[ 3.,  4.]]]\n",
    "res = tf.slice(x, [0, 1, 0], [1, 1, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.Session() as sess:\n",
    "        sess.run(res)\n",
    "        print(res.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_filename = '/home/ubuntu/projects/datasets/LibriSpeech/test-clean-wav/7127-75947-0000.wav'\n",
    "noise_filename = \"/home/ubuntu/projects/datasets/noise_wav/00noice.wav\"\n",
    "!sox --i \"/home/ubuntu/projects/datasets/noise_wav/00noice.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tf.read_file(wav_filename)\n",
    "noise_samples = tf.read_file(noise_filename)\n",
    "decoded = contrib_audio.decode_wav(samples, desired_channels=1)\n",
    "noise_decoded = contrib_audio.decode_wav(noise_samples, desired_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.size(decoded.audio) > tf.size(noise_decoded.audio) \n",
    "\n",
    "#tf.greater(tf.size(decoded.audio) , tf.size(noise_decoded.audio) )\n",
    "\n",
    "# and tf.size(decoded.sample_rate) != tf.size(noise_decoded.sample_rate)\n",
    "#decoded_audio = tf.cond(tf.size(decoded.audio) > tf.size(noise_decoded.audio), lambda : decoded.audio, lambda : tf.add(decoded.audio+noise_decoded.audio[:len(decoded.audio.eval())]))\n",
    "\n",
    "\n",
    "#transcript = \"and you know it\"\n",
    "#tf.SparseTensor(*transcript)\n",
    "#transcript = tf.constant(transcript)\n",
    "#tf.SparseTensor(*transcript)\n",
    "tf.math.equal(decoded.sample_rate , noise_decoded.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from DeepSpeech import create_inference_graph\n",
    "from util.config import  Config, initialize_globals\n",
    "from util.flags import create_flags, FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_flags()\n",
    "#initialize_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-272474fd3507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minitialize_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/DeepSpeech/util/config.py\u001b[0m in \u001b[0;36minitialize_globals\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Set default dropout rates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_rate2\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_rate2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_rate3\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 633\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "initialize_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session(config=Config.session_config) as session:\n",
    "inputs, outputs, _ = create_inference_graph(batch_size=1, n_steps=-1)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "almondaiEnv",
   "language": "python",
   "name": "almondaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
